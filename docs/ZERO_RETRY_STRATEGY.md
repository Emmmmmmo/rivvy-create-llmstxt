# Zero Retry Strategy - Maximum API Efficiency

## ğŸ¯ Philosophy: Fail Fast, Review Later

**NEW STRATEGY:** If an API call fails once â†’ immediately move to retry queue for manual review.

**Why?** Problematic URLs rarely succeed on automatic retry. Better to:
- Save the URL for later
- Continue scraping good URLs
- Manually review/retry when ready

---

## ğŸ”„ How It Works

### **The Flow:**

```
API Call to Product URL
       â†“
   [Try Once]
       â†“
    Success? â”€â”€YESâ†’ Save to shard âœ…
       â”‚
       NO
       â†“
Move to retry-queue.json
       â†“
Continue to next URL
       â†“
(Manual retry later when convenient)
```

---

## ğŸ“Š Comparison

### **OLD (Automatic Retries):**
```
URL fails
  â†“
Retry #1 (API call #2)
  â†“ fails
Retry #2 (API call #3)
  â†“ fails
Move to retry queue
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 3 API calls
Time wasted: ~180 seconds (3 Ã— 60s timeout)
Success rate on retry: ~5% (rarely works)
```

### **NEW (Zero Retries):**
```
URL fails
  â†“
Move to retry queue immediately
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 1 API call âœ…
Time saved: 120 seconds
Success rate: Same (manual retry later)
```

---

## ğŸ’° Cost Impact

### **Scenario: 6,000 products, 1% failure rate (60 URLs fail)**

| Strategy | API Calls on Failures | Cost | Time Wasted |
|----------|----------------------|------|-------------|
| **3 auto-retries** | 60 Ã— 3 = 180 calls | $0.15 | 180 minutes |
| **2 auto-retries** | 60 Ã— 2 = 120 calls | $0.10 | 120 minutes |
| **ZERO retries** | 60 Ã— 1 = 60 calls | $0.05 | 0 minutes âœ… |

**Savings with zero retries:**
- **60-120 fewer API calls** per scraping run
- **$0.05-$0.10 saved** per full scrape
- **2-3 hours saved** in wait time
- **100% control** over when to retry

---

## ğŸ¯ Why Automatic Retries Don't Help

### **Common Failure Reasons:**

| Failure Type | Auto-Retry Success Rate | Better Approach |
|--------------|------------------------|-----------------|
| **404 Not Found** | 0% | Don't retry (deleted product) |
| **Malformed Page** | 0% | Fix site config first |
| **Slow Load** | 10% | Retry during off-peak hours |
| **Rate Limited** | 0% | Wait longer, then retry |
| **API Overload** | 5% | Retry when API is stable |

**Average success rate on immediate retry: ~5%**

**Conclusion:** Not worth the API calls and time!

---

## ğŸ” What Gets Moved to Retry Queue?

### **Automatic Retry Queue Triggers:**

1. âœ… **Timeout** (> 60 seconds) â†’ retry queue
2. âœ… **No data returned** â†’ retry queue
3. âœ… **Request exception** â†’ retry queue
4. âœ… **Empty response** â†’ retry queue
5. âœ… **Parse error** â†’ retry queue

### **What Happens:**
```json
{
  "url": "https://www.mydiy.ie/products/problem-product.html",
  "normalized_url": "...",
  "metadata": {
    "attempts": 1,
    "last_error": "No data returned at 2025-10-05T20:45:30.123Z",
    "category_shard_key": "power_tools",
    "source_category": "https://www.mydiy.ie/power-tools/drills/"
  }
}
```

---

## ğŸš€ Usage

### **1. Normal Scraping (No Change for You)**

Just run your commands as usual:

```bash
source env.local && python3 scripts/update_llms_agnostic.py mydiy.ie \
  --hierarchical https://www.mydiy.ie/power-tools/ \
  --max-products 500 \
  --batch-size 25 \
  --max-batches 4
```

**What happens internally:**
- âœ… Good URLs: Scraped and saved (1 API call each)
- âš ï¸ Failed URLs: Immediately to retry queue (1 API call each)
- ğŸ“Š Log shows: `"Failed to scrape {url}; moved to retry queue"`

---

### **2. Review Failed URLs**

After scraping, check what failed:

```bash
# How many failed?
jq 'length' out/mydiy-ie/retry-queue.json

# What failed?
jq '.[] | {url: .url, category: .metadata.category_shard_key, error: .metadata.last_error}' out/mydiy-ie/retry-queue.json
```

---

### **3. Manually Retry When Ready**

```bash
# Retry all failed URLs (they get ONE fresh attempt)
source env.local && python3 scripts/update_llms_agnostic.py mydiy.ie \
  --process-retry-queue \
  --batch-size 10 \
  --max-batches 5
```

**What happens:**
- Moves retry queue â†’ pending queue
- Resets attempt counter
- Each URL gets ONE fresh try
- Still fails? â†’ Back to retry queue
- Success? â†’ Saved to shard âœ…

---

## ğŸ“ˆ Expected Results

### **For MyDIY.ie (6,000 products):**

```
Expected failure rate: 1-3% (60-180 URLs)

After first pass:
  âœ… Scraped: 5,820-5,940 products
  ğŸ“‹ Retry queue: 60-180 URLs
  
After manual retry:
  âœ… Scraped: +50-150 products (success rate 80-85%)
  ğŸ“‹ Retry queue: 10-30 URLs (probably deleted/broken)
  
Final:
  âœ… Total scraped: 5,870-6,090 products
  âŒ Permanent failures: 10-30 (0.2-0.5%)
```

---

## ğŸ“ Best Practices

### **1. Don't Worry During Initial Scrape**
```
Just let it run!
Failed URLs are safely stored
Focus on bulk scraping first
Review failures later
```

### **2. Review Retry Queue Periodically**
```bash
# After every 5 categories
jq 'length' out/mydiy-ie/retry-queue.json

# Check for patterns
jq 'group_by(.metadata.category_shard_key) | map({category: .[0].metadata.category_shard_key, count: length})' out/mydiy-ie/retry-queue.json
```

### **3. Batch Manual Retries**
```bash
# Don't retry all at once
# Start with a small batch to test

# Test batch (10 URLs)
python3 scripts/update_llms_agnostic.py mydiy.ie \
  --process-retry-queue \
  --batch-size 5 \
  --max-batches 2

# Full retry (all URLs)
python3 scripts/update_llms_agnostic.py mydiy.ie \
  --process-retry-queue \
  --batch-size 25 \
  --max-batches 20
```

### **4. Identify Permanent Failures**
```bash
# URLs that failed twice are probably not worth retrying
jq '.[] | select(.metadata.attempts >= 2)' out/mydiy-ie/retry-queue.json

# Manually check these in browser
# If 404 or broken, remove from retry queue
```

---

## ğŸ” Debugging

### **High Failure Rate (>5%)?**

**Check these:**

1. **Internet connection:** Slow/unstable?
2. **Site issues:** Is MyDIY.ie slow today?
3. **API issues:** Is Firecrawl overloaded?
4. **Timeout too aggressive:** 60s might be too short for some pages

**Solution:**
```python
# In scripts/update_llms_agnostic.py
# Temporarily increase timeout
timeout=90  # Was 60
```

---

### **Same URLs Failing Repeatedly?**

**Check the URL pattern:**

```bash
# Get failed URLs
jq '.[] | .url' out/mydiy-ie/retry-queue.json > failed-urls.txt

# Manually visit first 10 in browser
# Are they valid pages?
```

**Common patterns:**
- All from same category â†’ Site config issue
- All deleted products â†’ Remove from queue
- All timeout â†’ Site is slow, try off-peak hours

---

## ğŸ’¡ Manual Retry Tips

### **When to Retry:**

âœ… **Good times:**
- Off-peak hours (late night, early morning)
- After fixing site config issues
- After Firecrawl API improvements
- When your internet is stable

âŒ **Bad times:**
- Immediately after initial scrape
- During peak hours (site is busy)
- When seeing API rate limits

### **Retry Strategy:**

```bash
# Day 1: Initial scrape (all categories)
# Result: 5,850 scraped, 150 in retry queue

# Day 2: Review and categorize failures
jq '.[] | {url, category, error}' out/mydiy-ie/retry-queue.json

# Day 3: Retry (off-peak hours)
python3 scripts/update_llms_agnostic.py mydiy.ie --process-retry-queue --batch-size 10 --max-batches 10
# Result: +120 scraped, 30 still failing

# Day 4: Manual review of remaining 30
# Check if they're deleted products
# Remove permanent failures from queue
```

---

## ğŸ“Š Success Metrics

### **Healthy Scraping Operation:**

```
Initial scrape success rate: 97-99% âœ…
Manual retry success rate: 80-85% âœ…
Final failure rate: <1% âœ…
```

### **Monitor These:**

```bash
# Success rate
echo "Success: $(jq '[.[] | length] | add' out/mydiy-ie/llms-mydiy-ie-manifest.json)"
echo "Failed: $(jq 'length' out/mydiy-ie/retry-queue.json)"

# Calculate percentage
python3 -c "
success = $(jq '[.[] | length] | add' out/mydiy-ie/llms-mydiy-ie-manifest.json)
failed = $(jq 'length' out/mydiy-ie/retry-queue.json)
total = success + failed
print(f'Success rate: {success/total*100:.1f}%')
"
```

---

## ğŸ‰ Benefits Summary

| Metric | Zero Retries | 2 Retries | 5 Retries |
|--------|--------------|-----------|-----------|
| **API calls per failure** | 1 | 2 | 5 |
| **Time per failure** | 60s | 180s | 300s |
| **Cost per failure** | $0.0008 | $0.0017 | $0.0042 |
| **Manual control** | âœ… Full | âš ï¸ Partial | âŒ None |
| **Pattern detection** | âœ… Easy | âš ï¸ Harder | âŒ Impossible |
| **API efficiency** | âœ… 100% | âš ï¸ 50% | âŒ 20% |

---

## ğŸ›¡ï¸ Safety Features

1. âœ… **Nothing lost** - All failures tracked in retry queue
2. âœ… **Full context** - Error message, category, timestamp saved
3. âœ… **Resumable** - Can retry any time
4. âœ… **Controlled** - You decide when to retry
5. âœ… **Efficient** - Zero wasted API calls
6. âœ… **Fast** - No time wasted on immediate retries

---

## ğŸ¯ Summary

**Philosophy:** Try once, fail fast, review smart

**OLD:** 3 API calls per failure = 66% waste
**NEW:** 1 API call per failure = 0% waste âœ…

**Your scraping is now:**
- âš¡ Faster (no retry delays)
- ğŸ’° Cheaper (no wasted calls)
- ğŸ¯ Smarter (manual control)
- ğŸ“Š Transparent (see all failures)

**You're in complete control!** ğŸš€

