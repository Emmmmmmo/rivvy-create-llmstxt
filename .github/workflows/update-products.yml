name: Update Product Data

on:
  repository_dispatch:
    types: [website_changed]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.event.client_payload.website.id || github.run_id }}
  cancel-in-progress: true

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Debug trigger information
        run: |
          echo "Event name: ${{ github.event_name }}"
          echo "Action: ${{ github.event.action }}"
          echo "Actor: ${{ github.actor }}"
          echo "Ref: ${{ github.ref }}"
          echo "SHA: ${{ github.sha }}"
          echo "Run ID: ${{ github.run_id }}"
          echo "Run number: ${{ github.run_number }}"
          
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          
      - name: Validate webhook payload
        if: github.event_name == 'repository_dispatch'
        run: |
          echo "Validating rivvy-observer webhook payload..."
          echo "Event type: ${{ github.event_name }}"
          echo "Action: ${{ github.event.action }}"
          echo "Payload received successfully"
          
          # Write payload to temp file for safe processing (use cat to avoid bash parsing issues)
          cat > /tmp/payload.json << 'WEBHOOK_PAYLOAD_EOF'
          ${{ toJson(github.event.client_payload) }}
          WEBHOOK_PAYLOAD_EOF
          
          # Validate rivvy-observer format - check for new structure
          echo "Validating required fields..."
          
          # Check for website.url
          website_url=$(jq -r '.website.url // empty' /tmp/payload.json)
          if [ -z "$website_url" ]; then
            echo "Error: Missing 'website.url' field in rivvy-observer payload"
            exit 1
          fi
          
          # Check for changedPages array (new format)
          changed_pages_count=$(jq -r '.changedPages | length // 0' /tmp/payload.json)
          if [ "$changed_pages_count" -gt 0 ]; then
            echo "New payload format detected with $changed_pages_count changed pages"
            echo "Website URL: $website_url"
            
            # Validate each changed page has required fields
            for i in $(seq 0 $((changed_pages_count - 1))); do
              page_url=$(jq -r ".changedPages[$i].url // empty" /tmp/payload.json)
              change_type=$(jq -r ".changedPages[$i].changeType // empty" /tmp/payload.json)
              
              if [ -z "$page_url" ] || [ -z "$change_type" ]; then
                echo "Error: Missing required fields in changedPages[$i]"
                exit 1
              fi
              
              echo "Page $((i+1)): $page_url ($change_type)"
            done
          else
            # Check for legacy single-page format
            change_type=$(jq -r '.change.changeType // empty' /tmp/payload.json)
            if [ -z "$change_type" ]; then
              echo "Error: Missing 'change.changeType' or 'changedPages' in payload"
              exit 1
            fi
            echo "Legacy payload format detected"
            echo "Website URL: $website_url"
            echo "Change Type: $change_type"
          fi
          
          echo "rivvy-observer payload validation passed"

      - name: Update products
        env:
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
        run: |
          echo "Action: ${{ github.event.action }}"
          
          # Check if this is a repository_dispatch event
          if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
            echo "Processing rivvy-observer webhook..."
            
            # Extract site info from rivvy-observer payload (already written to temp file in validation step)
            site_url=$(jq -r '.website.url' /tmp/payload.json)
            
            # Extract domain for output directory
            domain=$(echo "$site_url" | sed -E 's|^https?://||' | sed -E 's|^www\.||' | sed -E 's|/.*$||' | sed -E 's|:.*$||')
            
            if [ -z "$domain" ]; then
              echo "Error: Could not extract domain from: $site_url"
              exit 1
            fi
            
            output_dir="out/$domain"
            echo "Domain: $domain"
            echo "Output directory: $output_dir"
            
            # Ensure output directory exists
            mkdir -p "$output_dir"
            
            # Check for new multi-page format
            changed_pages_count=$(jq -r '.changedPages | length // 0' /tmp/payload.json)
            
            if [ "$changed_pages_count" -gt 0 ]; then
              echo "Processing $changed_pages_count changed pages..."
              
              # Arrays to collect URLs for batch processing
              added_urls=()
              removed_urls=()
              
              # Process each changed page
              for i in $(seq 0 $((changed_pages_count - 1))); do
                page_url=$(jq -r ".changedPages[$i].url" /tmp/payload.json)
                change_type=$(jq -r ".changedPages[$i].changeType" /tmp/payload.json)
                scraped_content=$(jq -r ".changedPages[$i].scrapedContent.markdown // empty" /tmp/payload.json)
                diff_text=$(jq -r ".changedPages[$i].diff.text // empty" /tmp/payload.json)
                
                echo "Processing page $((i+1))/$changed_pages_count: $page_url ($change_type)"
                
                case "$change_type" in
                  "page_added")
                    added_urls+=("$page_url")
                    
                    # For page_added, check if there's diff.text to extract only the new content
                    if [ -n "$diff_text" ] && [ "$diff_text" != "null" ] && [ "$diff_text" != "empty" ]; then
                      # Create a safe filename by hashing the URL
                      url_hash=$(echo "$page_url" | md5sum | cut -d' ' -f1)
                      diff_file="/tmp/diff_content_${i}_${url_hash}.md"
                      printf '%s\n' "$diff_text" > "$diff_file"
                      echo "Saved diff content for $page_url (page_added with diff)"
                      
                      # Mark this as a diff file for processing
                      echo "$page_url" >> /tmp/diff_urls.txt
                    elif [ -n "$scraped_content" ] && [ "$scraped_content" != "null" ]; then
                      # Fallback: use full scraped content if no diff.text available
                      url_hash=$(echo "$page_url" | md5sum | cut -d' ' -f1)
                      temp_file="/tmp/scraped_content_${i}_${url_hash}.md"
                      printf '%s\n' "$scraped_content" > "$temp_file"
                      echo "Saved scraped content for $page_url (page_added without diff)"
                    fi
                    ;;
                  "content_modified"|"content_changed")
                    added_urls+=("$page_url")
                    
                    # For content modifications, check if there's diff.text first
                    if [ -n "$diff_text" ] && [ "$diff_text" != "null" ] && [ "$diff_text" != "empty" ]; then
                      # Create a safe filename by hashing the URL
                      url_hash=$(echo "$page_url" | md5sum | cut -d' ' -f1)
                      diff_file="/tmp/diff_content_${i}_${url_hash}.md"
                      printf '%s\n' "$diff_text" > "$diff_file"
                      echo "Saved diff content for $page_url"
                      
                      # Mark this as a diff file for processing
                      echo "$page_url" >> /tmp/diff_urls.txt
                    elif [ -n "$scraped_content" ] && [ "$scraped_content" != "null" ]; then
                      # Fallback: use full scraped content if no diff available
                      url_hash=$(echo "$page_url" | md5sum | cut -d' ' -f1)
                      temp_file="/tmp/scraped_content_${i}_${url_hash}.md"
                      printf '%s\n' "$scraped_content" > "$temp_file"
                      echo "Saved scraped content for $page_url"
                    fi
                    ;;
                  "page_removed")
                    removed_urls+=("$page_url")
                    
                    # For page_removed, save diff content if available for extraction
                    if [ -n "$diff_text" ] && [ "$diff_text" != "null" ] && [ "$diff_text" != "empty" ]; then
                      # Create a safe filename by hashing the URL
                      url_hash=$(echo "$page_url" | md5sum | cut -d' ' -f1)
                      diff_file="/tmp/diff_content_${i}_${url_hash}.md"
                      printf '%s\n' "$diff_text" > "$diff_file"
                      echo "Saved diff content for $page_url (page_removed with diff)"
                      
                      # Mark this as a diff file for processing
                      echo "$page_url" >> /tmp/diff_urls.txt
                    fi
                    ;;
                  *)
                    echo "Unknown change type: $change_type, treating as content modification"
                    added_urls+=("$page_url")
                    ;;
                esac
              done
              
              # Process added/modified pages in batch
              if [ ${#added_urls[@]} -gt 0 ]; then
                echo "Processing ${#added_urls[@]} added/modified pages..."
                added_urls_json=$(printf '%s\n' "${added_urls[@]}" | jq -R . | jq -s .)
                echo "Added URLs: $added_urls_json"
                
                # Check if there are any diff-based URLs to process
                if [ -f "/tmp/diff_urls.txt" ]; then
                  echo "Detected diff-based content for incremental processing"
                  
                  # Find the first diff file to pass as pre-scraped content
                  first_diff_file=$(ls -1 /tmp/diff_content_*.md 2>/dev/null | head -1)
                  
                  if [ -n "$first_diff_file" ] && [ -f "$first_diff_file" ]; then
                    echo "Using diff file: $first_diff_file"
                    # Use agnostic scraping system with diff mode and diff content
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "$added_urls_json" \
                      --use-diff-extraction \
                      --pre-scraped-content "$first_diff_file"
                  else
                    echo "No diff file found, using flag only"
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "$added_urls_json" \
                      --use-diff-extraction
                  fi
                else
                  # Use agnostic scraping system normally
                  echo "Using agnostic scraping system for $domain"
                  python3 scripts/update_llms_agnostic.py "$domain" \
                    --added "$added_urls_json"
                fi
              fi
              
              # Process removed pages in batch
              if [ ${#removed_urls[@]} -gt 0 ]; then
                echo "Processing ${#removed_urls[@]} removed pages..."
                removed_urls_json=$(printf '%s\n' "${removed_urls[@]}" | jq -R . | jq -s .)
                echo "Removed URLs: $removed_urls_json"

                # If any diff files were captured, pass the first one as --diff-file
                if [ -f "/tmp/diff_urls.txt" ]; then
                  echo "Detected diff-based content for removals"
                  first_diff_file=$(ls -1 /tmp/diff_content_*.md 2>/dev/null | head -1)
                  if [ -n "$first_diff_file" ] && [ -f "$first_diff_file" ]; then
                    echo "Using diff file: $first_diff_file for removals"
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --removed "$removed_urls_json" \
                      --diff-file "$first_diff_file"
                  else
                    echo "No diff file found for removals; proceeding without diff"
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --removed "$removed_urls_json"
                  fi
                else
                  # No diff info collected; proceed without diff
                  python3 scripts/update_llms_agnostic.py "$domain" \
                    --removed "$removed_urls_json"
                fi
              fi
              
              # Clean up temp files
              rm -f /tmp/scraped_content_*.md /tmp/diff_content_*.md /tmp/diff_urls.txt
              
            else
              # Legacy single-page format
              echo "Processing legacy single-page format..."
              
              change_type=$(jq -r '.change.changeType' /tmp/payload.json)
              scraped_content=$(jq -r '.scrapeResult.markdown // empty' /tmp/payload.json)
              diff_text=$(jq -r '.change.diff.text // empty' /tmp/payload.json)
              
              echo "Site URL: $site_url"
              echo "Change type: $change_type"
              echo "Has scraped content: $([ -n "$scraped_content" ] && [ "$scraped_content" != "null" ] && echo "Yes" || echo "No")"
              echo "Has diff text: $([ -n "$diff_text" ] && [ "$diff_text" != "null" ] && [ "$diff_text" != "empty" ] && echo "Yes" || echo "No")"
              
              # Process based on change type
              case "$change_type" in
                "page_added")
                  echo "Processing page_added for $domain"
                  echo "Using agnostic scraping system for $domain"
                  
                  # Check if there's diff.text to extract only the new content
                  if [ -n "$diff_text" ] && [ "$diff_text" != "null" ] && [ "$diff_text" != "empty" ]; then
                    # Create temp file with diff content
                    diff_file="/tmp/diff_content_$domain.md"
                    printf '%s\n' "$diff_text" > "$diff_file"
                    echo "Using diff extraction for new product only"
                    
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "[\"$site_url\"]" \
                      --pre-scraped-content "$diff_file" \
                      --use-diff-extraction
                  elif [ -n "$scraped_content" ] && [ "$scraped_content" != "null" ]; then
                    # Fallback: use full scraped content
                    temp_file="/tmp/scraped_content_$domain.md"
                    printf '%s\n' "$scraped_content" > "$temp_file"
                    echo "No diff available, using full scraped content"
                    
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "[\"$site_url\"]" \
                      --pre-scraped-content "$temp_file"
                  else
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "[\"$site_url\"]"
                  fi
                  ;;
                "content_modified"|"content_changed")
                  echo "Processing content modification for $domain"
                  echo "Using agnostic scraping system for $domain"
                  
                  # Check if there's diff.text for incremental updates
                  if [ -n "$diff_text" ] && [ "$diff_text" != "null" ] && [ "$diff_text" != "empty" ]; then
                    # Create temp file with diff content
                    diff_file="/tmp/diff_content_$domain.md"
                    printf '%s\n' "$diff_text" > "$diff_file"
                    echo "Using diff extraction for changes"
                    
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "[\"$site_url\"]" \
                      --pre-scraped-content "$diff_file" \
                      --use-diff-extraction
                  elif [ -n "$scraped_content" ] && [ "$scraped_content" != "null" ]; then
                    # Fallback: use full scraped content
                    temp_file="/tmp/scraped_content_$domain.md"
                    printf '%s\n' "$scraped_content" > "$temp_file"
                    
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "[\"$site_url\"]" \
                      --pre-scraped-content "$temp_file"
                  else
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "[\"$site_url\"]"
                  fi
                  ;;
                "page_removed")
                  echo "Processing page removal for $domain"
                  
                  # Check if there's diff.text for extraction
                  if [ -n "$diff_text" ] && [ "$diff_text" != "null" ] && [ "$diff_text" != "empty" ]; then
                    # Create temp file with diff content
                    diff_file="/tmp/diff_content_$domain.md"
                    printf '%s\n' "$diff_text" > "$diff_file"
                    echo "Using diff extraction for removed products"
                    
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --removed "[\"$site_url\"]" \
                      --diff-file "$diff_file"
                  else
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --removed "[\"$site_url\"]"
                  fi
                  ;;
                *)
                  echo "Unknown change type: $change_type, treating as content modification"
                  echo "Using agnostic scraping system for $domain"
                  if [ -f "$temp_file" ]; then
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "[\"$site_url\"]" \
                      --pre-scraped-content "$temp_file"
                  else
                    python3 scripts/update_llms_agnostic.py "$domain" \
                      --added "[\"$site_url\"]"
                  fi
                  ;;
              esac
              
              # Clean up temp files
              rm -f /tmp/scraped_content_$domain.md /tmp/diff_content_$domain.md
            fi
            
            echo "Successfully processed webhook for $domain"
            echo "Output directory: $output_dir"
            echo "Files in output directory:"
            ls -la "$output_dir" || echo "No files found in output directory"
          else
            # For push events, dynamically discover and process all existing sites
            echo "Processing URLs from manifest files..."
            
            # Find all directories in out/ that contain manifest.json files
            for site_dir in out/*/; do
              if [ -d "$site_dir" ] && [ -f "${site_dir}manifest.json" ]; then
                # Extract domain from directory name
                domain=$(basename "$site_dir")
                manifest_file="${site_dir}manifest.json"
                
                echo "Processing $domain URLs from manifest..."
                
                # Extract URLs from manifest and process them
                if python3 -c "import json; data=json.load(open('$manifest_file')); print(json.dumps(data.get('collections', [])))" > /tmp/urls_$domain.json 2>/dev/null; then
                  urls=$(cat /tmp/urls_$domain.json)
                  
                  if [ "$urls" != "[]" ] && [ "$urls" != "null" ]; then
                    # Determine the base URL for the site
                    base_url="https://www.$domain"
                    
                    # Try to get the actual site URL from manifest if available
                    if python3 -c "import json; data=json.load(open('$manifest_file')); print(data.get('base_url', ''))" > /tmp/base_url_$domain.txt 2>/dev/null; then
                      manifest_base_url=$(cat /tmp/base_url_$domain.txt | tr -d '"')
                      if [ -n "$manifest_base_url" ] && [ "$manifest_base_url" != "null" ]; then
                        base_url="$manifest_base_url"
                      fi
                    fi
                    
                    echo "Using base URL: $base_url for domain: $domain"
                    python3 scripts/update_llms_sharded.py "$base_url" --added "$urls" --output-dir "$site_dir"
                    
                    # Clean up temp files
                    rm -f /tmp/urls_$domain.json /tmp/base_url_$domain.txt
                  else
                    echo "No URLs found in manifest for $domain"
                  fi
                else
                  echo "Error: Failed to read manifest file for $domain"
                fi
              fi
            done
          fi
          
      - name: Check for changes
        id: changes
        run: |
          if [ -n "$(git status --porcelain)" ]; then
            echo "changes=true" >> $GITHUB_OUTPUT
          else
            echo "changes=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Commit changes
        if: steps.changes.outputs.changes == 'true'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .
          git commit -m "Auto-update: ${{ github.event_name }} - ${{ github.event.action || 'push' }}"
          # Handle concurrent pushes by pulling first
          git pull --rebase origin main
          git push origin main
          
      - name: Sync to ElevenLabs
        if: steps.changes.outputs.changes == 'true'
        env:
          ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}
        run: |
          echo "Files updated, syncing to ElevenLabs..."
          
          # Check if ElevenLabs API key is available
          if [ -z "$ELEVENLABS_API_KEY" ]; then
            echo "Warning: ELEVENLABS_API_KEY not set, skipping ElevenLabs sync"
            exit 0
          fi
          
          # Check if agent config exists
          if [ ! -f "config/elevenlabs-agents.json" ]; then
            echo "Warning: config/elevenlabs-agents.json not found, skipping ElevenLabs sync"
            exit 0
          fi
          
          # Extract domain from changed files to determine which domain to sync
          changed_domains=$(git diff --name-only HEAD~1 | grep "^out/" | cut -d'/' -f2 | sort -u)
          
          if [ -z "$changed_domains" ]; then
            echo "No domain changes detected, discovering all domains..."
            # Find all domains in out/ directory
            for domain_dir in out/*/; do
              if [ -d "$domain_dir" ]; then
                domain=$(basename "$domain_dir")
                echo "Syncing domain: $domain"
                python3 scripts/knowledge_base_manager.py sync --domain "$domain"
              fi
            done
          else
            echo "Detected changes in domains: $changed_domains"
            for domain in $changed_domains; do
              echo "Syncing domain: $domain"
              python3 scripts/knowledge_base_manager.py sync --domain "$domain"
            done
          fi
          
          echo "ElevenLabs sync completed successfully"
          
      - name: Commit sync state changes
        if: steps.changes.outputs.changes == 'true'
        run: |
          # Check if sync state file was modified
          if [ -n "$(git status --porcelain config/elevenlabs_sync_state.json)" ]; then
            echo "Sync state file was updated, committing changes..."
            git config --local user.email "action@github.com"
            git config --local user.name "GitHub Action"
            git add config/elevenlabs_sync_state.json
            git commit -m "Update ElevenLabs sync state after sync operation"
            git pull --rebase origin main
            git push origin main
            echo "Sync state committed successfully"
          else
            echo "No sync state changes to commit"
          fi